{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27650c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daniëlla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daniëlla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Daniëlla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Daniëlla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Daniëlla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa135f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7003e2",
   "metadata": {},
   "source": [
    "## Sentiment analyse reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fec3629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tussenstappen:\n",
      "Original text: I love this show.\n",
      "Tokens: ['I', 'love', 'this', 'show', '.']\n",
      "Tokens after removing stop words: ['love', 'show', '.']\n",
      "Tokens after lemmatization: ['love', 'show', '.']\n",
      "Preprocessed text: love show .\n",
      "\n",
      "Sentiment analysis for Corpus 1:\n",
      "Positive sentiment: love show .\n",
      "Positive sentiment: love movie , hate one character .\n",
      "Positive sentiment: hate character love season last one .\n",
      "\n",
      "Sentiment analysis for Corpus 2:\n",
      "Negative sentiment: worst movie ever watched .\n",
      "Negative sentiment: suck as .\n",
      "Negative sentiment: Everything movie suck except main character .\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Step 2: Remove stop words\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Step 3: Lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Step 4: Sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    sentiment = sentiment_scores['compound']\n",
    "    return sentiment\n",
    "\n",
    "# Step 5: Preprocessing pipeline\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = remove_stop_words(tokens)\n",
    "    lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Example usage\n",
    "corpus1 = [\"I love this show.\",\n",
    "           \"i love this movie, but i hate this one character.\",\n",
    "           \"i hate this character but love this season more than the last one.\"]\n",
    "\n",
    "corpus2 = [\"This is the worst movie i have ever watched.\",\n",
    "           \"This sucks ass.\",\n",
    "           \"Everything about this movie sucks except the main character.\"]\n",
    "\n",
    "preprocessed_corpus1 = [preprocess_text(text) for text in corpus1]\n",
    "preprocessed_corpus2 = [preprocess_text(text) for text in corpus2]\n",
    "\n",
    "# Tussenstappen printen\n",
    "print(\"Tussenstappen:\")\n",
    "print(\"Original text:\", corpus1[0])\n",
    "tokens = tokenize_text(corpus1[0])\n",
    "print(\"Tokens:\", tokens)\n",
    "filtered_tokens = remove_stop_words(tokens)\n",
    "print(\"Tokens after removing stop words:\", filtered_tokens)\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Tokens after lemmatization:\", lemmatized_tokens)\n",
    "preprocessed_text = preprocess_text(corpus1[0])\n",
    "print(\"Preprocessed text:\", preprocessed_text)\n",
    "print()\n",
    "\n",
    "# Analyze sentiment for corpus 1\n",
    "print(\"Sentiment analysis for Corpus 1:\")\n",
    "for text in preprocessed_corpus1:\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    if sentiment >= 0.05:\n",
    "        print(f\"Positive sentiment: {text}\")\n",
    "    elif sentiment <= -0.05:\n",
    "        print(f\"Negative sentiment: {text}\")\n",
    "    else:\n",
    "        print(f\"Neutral sentiment: {text}\")\n",
    "\n",
    "# Analyze sentiment for corpus 2\n",
    "print(\"\\nSentiment analysis for Corpus 2:\")\n",
    "for text in preprocessed_corpus2:\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    if sentiment >= 0.05:\n",
    "        print(f\"Positive sentiment: {text}\")\n",
    "    elif sentiment <= -0.05:\n",
    "        print(f\"Negative sentiment: {text}\")\n",
    "    else:\n",
    "        print(f\"Neutral sentiment: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07859b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment for Corpus 1: Positive sentiment\n",
      "Average sentiment for Corpus 2: Negative sentiment\n",
      "['Positive sentiment', 'Negative sentiment']\n"
     ]
    }
   ],
   "source": [
    "def average_sentiment(corpus):\n",
    "    sentiments = [analyze_sentiment(text) for text in corpus]\n",
    "    average_sentiment = sum(sentiments) / len(sentiments)\n",
    "    if average_sentiment >= 0.05:\n",
    "        average_sentiment = \"Positive sentiment\"\n",
    "    elif average_sentiment <= -0.05:\n",
    "        average_sentiment = \"Negative sentiment\"\n",
    "    else:    \n",
    "        average_sentiment = \"Neutral sentiment\"\n",
    "    return average_sentiment\n",
    "\n",
    "\n",
    "\n",
    "# average sentiment corpus 1\n",
    "average_sentiment1 = average_sentiment(preprocessed_corpus1)\n",
    "print(\"Average sentiment for Corpus 1:\", average_sentiment1)\n",
    "\n",
    "\n",
    "# average sentiment corpus 2\n",
    "average_sentiment2 = average_sentiment(preprocessed_corpus2)\n",
    "print(\"Average sentiment for Corpus 2:\", average_sentiment2)\n",
    "\n",
    "#multple corpus average sentiment at the same time plus adding the average sentiment to a dataframe\n",
    "corpus = [preprocessed_corpus1, preprocessed_corpus2]\n",
    "average_sentiments = [average_sentiment(text) for text in corpus]\n",
    "print(average_sentiments)\n",
    "\n",
    "df = pd.DataFrame({'corpus': corpus, 'average_sentiment': average_sentiments})\n",
    "df\n",
    "\n",
    "#make index number of corpus\n",
    "df.index = ['corpus1', 'corpus2']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a16b332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>corpus1</th>\n",
       "      <td>Positive sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corpus2</th>\n",
       "      <td>Negative sentiment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          average_sentiment\n",
       "corpus1  Positive sentiment\n",
       "corpus2  Negative sentiment"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete corpus column\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb9917",
   "metadata": {},
   "source": [
    "### Probleemstelling\n",
    "\n",
    "Netflix maakt regelmatig beslissingen over de toekomst van series en films. Een productie kan soms meerdere seizoenen of vervolgfilms in de planning hebben staan, maar toch worden die dan geannuleerd. Dan kan de vraag ontstaan of het annuleren wel de juiste keuze was. \n",
    "\n",
    "Het annuleren van een vervolg op een film of serie wordt negatief ervaren door fans en kan de reputatie van Netflix verminderen, wat kan leiden tot minder abonnees en lagere kijkcijfers doordat niemand aan nieuwe shows of films begint.\n",
    "\n",
    "Door middel van een onderzoek naar kijkcijfers, reviews en ratings van de eerdere delen, en reacties van het publiek wil Netflix erachter komen of het annuleren de juiste beslissing was. De taak van het machine learning team is om de reviews om te werken naar sentimenten om deze te kunnen gebruiken in een machine learning model. En een dashboard te maken waarmee de statistieken van een extra seizoen voorspeld kunnen worden.\n",
    "\n",
    "#### Zakelijk inzicht  \n",
    "Met behulp van dit onderzoek kan er worden gekeken naar de mogelijke performance van een volgend seizoen of vervolgfilm. Dit kan Netflix ondersteunen met onderbouwde keuzes maken, richting het annuleren van vervolgen om fans en abonnees tevreden te houden. Door te kijken naar de sentimenten, ratings en kijkcijfers kunnen zowel de bedrijfswinst en reputatie bij abonnees worden gewaarborgd. \n",
    "\n",
    "#### Literatuur onderzoek\n",
    "Het onderzoek richt zich op sentiment analyse met behulp van Natural Language Processing (NLP), hiermee kunnen reviews van rotten tomatoes worden omgezet naar sentimenten. Na onderzoek is geen soort gelijk project waarmee het onderzoek vergeleken kan worden. \n",
    "\n",
    "Netflix is een online streaming service, via deze service kan eigen media en ook overgekochte media worden gestreamd. In het geval van dit Onderzoek zal het over de eigen media gaan, de Netflix Orginals. De Netflix Originals worden gemaakt door de Netflix studios, bij hen ligt dus ook de keuzes om deze te annuleren of vervolgen. \n",
    "\n",
    "#### vereisten informatie\n",
    "- Een database met informatie en details van films en series van Netflix media.\n",
    "- De belangrijke details zijn onder andere aantal seizoenen, aantal kijkers, release jaar, ratings, genre, regisseur, cast.\n",
    "- De ratings moeten bruikbaar zijn bij machine learning.\n",
    "- De tweets moeten omgezet worden naar een formaat dat bruikbaar is zodat een machine learning model er op kan trainen.\n",
    "- De Rotten tomatoes website moet kunnen worden gescrapped \n",
    "- De API moet gebonden worden aan titels in de dataset\n",
    "\n",
    "#### Databronnen\n",
    "- Netflix series and movies Dataset: Netflix movies and TV shows. (2021, September 27). https://www.kaggle.com/datasets/shivamb/netflix-shows/data\n",
    "- Rotten tomatoes: Rotten Tomatoes: movies | TV shows | Movie trailers | reviews. (n.d.). Rotten Tomatoes. https://www.rottentomatoes.com/\n",
    "- API TMDB Movie database: Getting started. (n.d.). The Movie Database (TMDB). https://developer.themoviedb.org/reference/intro/getting-started\n",
    "\n",
    "#### Data verwerken\n",
    "- Dataset inladen naar DataFrame en klaar maken voor machine learning. Het verwijderen van de descriptions van de show/film en kolommen omzetten naar nummeriek bij bijvoorbeeld age rating een schaal van 1-5 maken.\n",
    "- De rotten tomatoes site wordt gescrapped voor reviews en ratings per show/film. Bij shows wordt ook gekeken naar het meest recente seizoen\n",
    "- Van TMDB wordt de data verzameld door middel van de API's\n",
    "\n",
    "#### Soort data\n",
    "- Dataset, gestructureerd: Er wordt gewerkt met een csv bestand waar elke rij een andere film/show betekent. \n",
    "- De rotten tomatoes, semi gestructureerd: De reviews kunnen worden gezien als ongestructureerd en de ratings als gestructureerd. Hierbij, dus de hele bron als semi gestructureerde data.\n",
    "- TMDB, semi gestructureerd: De informatie wordt behaald door API's te gebruiken. Deze API's worden aangeboden in een gestandaardiseerde vorm. Toch, wordt er voor Semi gestructureerd gekozen vanwege de gestructureerde en ongestructureerde variabele van dit format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
